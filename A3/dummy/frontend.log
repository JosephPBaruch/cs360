k3d is already installed.
k3d local registry 'k3d-local-registry' is already running. Deleting the existing registry...
[36mINFO[0m[0000] Creating node 'k3d-local-registry'           
[36mINFO[0m[0000] Successfully created registry 'k3d-local-registry' 
[36mINFO[0m[0000] Starting node 'k3d-local-registry'           
[36mINFO[0m[0000] Successfully created registry 'k3d-local-registry' 
# You can now use the registry like this (example):
# 1. create a new cluster that uses this registry
k3d cluster create --registry-use k3d-local-registry:5001

# 2. tag an existing local image to be pushed to the registry
docker tag nginx:latest k3d-local-registry:5001/mynginx:v0.1

# 3. push that image to the registry
docker push k3d-local-registry:5001/mynginx:v0.1

# 4. run a pod that uses this image
kubectl run mynginx --image k3d-local-registry:5001/mynginx:v0.1

The hostname 'k3d-local-registry' already exists in /etc/hosts.
[36mINFO[0m[0000] portmapping '8081:8081' targets the loadbalancer: defaulting to [servers:*:proxy agents:*:proxy] 
[36mINFO[0m[0000] portmapping '8082:8082' targets the loadbalancer: defaulting to [servers:*:proxy agents:*:proxy] 
[36mINFO[0m[0000] Prep: Network                                
[36mINFO[0m[0000] Created network 'k3d-dummy-cluster'          
[36mINFO[0m[0000] Created image volume k3d-dummy-cluster-images 
[36mINFO[0m[0000] Starting new tools node...                   
[36mINFO[0m[0000] Starting node 'k3d-dummy-cluster-tools'      
[36mINFO[0m[0001] Creating node 'k3d-dummy-cluster-server-0'   
[36mINFO[0m[0001] Creating node 'k3d-dummy-cluster-agent-0'    
[36mINFO[0m[0001] Creating node 'k3d-dummy-cluster-agent-1'    
[36mINFO[0m[0001] Creating LoadBalancer 'k3d-dummy-cluster-serverlb' 
[36mINFO[0m[0001] Using the k3d-tools node to gather environment information 
[36mINFO[0m[0001] Starting new tools node...                   
[36mINFO[0m[0001] Starting node 'k3d-dummy-cluster-tools'      
[36mINFO[0m[0002] Starting cluster 'dummy-cluster'             
[36mINFO[0m[0002] Starting servers...                          
[36mINFO[0m[0002] Starting node 'k3d-dummy-cluster-server-0'   
[36mINFO[0m[0006] Starting agents...                           
[36mINFO[0m[0006] Starting node 'k3d-dummy-cluster-agent-1'    
[36mINFO[0m[0006] Starting node 'k3d-dummy-cluster-agent-0'    
[36mINFO[0m[0011] Starting helpers...                          
[36mINFO[0m[0011] Starting node 'k3d-dummy-cluster-serverlb'   
[36mINFO[0m[0018] Injecting records for hostAliases (incl. host.k3d.internal) and for 6 network members into CoreDNS configmap... 
[36mINFO[0m[0023] Cluster 'dummy-cluster' created successfully! 
[36mINFO[0m[0024] You can now use it like this:                
kubectl cluster-info
k3d cluster 'dummy-cluster' is up and running.
You can now use kubectl to interact with your k3d cluster.
Building the client Docker image...
#0 building with "desktop-linux" instance using docker driver

#1 [internal] load .dockerignore
#1 transferring context: 2B done
#1 DONE 0.0s

#2 [internal] load build definition from Dockerfile
#2 transferring dockerfile: 673B done
#2 DONE 0.0s

#3 [internal] load metadata for docker.io/library/nginx:alpine
#3 ...

#4 [internal] load metadata for docker.io/library/node:18-alpine
#4 DONE 0.4s

#5 [stage-1 1/3] FROM docker.io/library/nginx:alpine@sha256:814a8e88df978ade80e584cc5b333144b9372a8e3c98872d07137dbf3b44d0e4
#5 DONE 0.0s

#3 [internal] load metadata for docker.io/library/nginx:alpine
#3 DONE 0.4s

#6 [builder 1/6] FROM docker.io/library/node:18-alpine@sha256:974afb6cbc0314dc6502b14243b8a39fbb2d04d975e9059dd066be3e274fbb25
#6 resolve docker.io/library/node:18-alpine@sha256:974afb6cbc0314dc6502b14243b8a39fbb2d04d975e9059dd066be3e274fbb25 0.1s done
#6 sha256:d59895120001b37ef5f75afc6342329f6037b1e2aea353a1055efa62b8bf6003 1.72kB / 1.72kB done
#6 sha256:7221f40791e5e6da0c9fa6b49b6238a51661222e4c58aba37e152e89914be09d 6.20kB / 6.20kB done
#6 sha256:974afb6cbc0314dc6502b14243b8a39fbb2d04d975e9059dd066be3e274fbb25 7.67kB / 7.67kB done
#6 sha256:e668eba0f82bcfec9fb0cd787bd7f3d013a1266567b092e90ff8b3d3be41807c 443B / 443B 0.5s done
#6 sha256:fc4eb59aab89271acc5a8bd8e5e96fc17af489976964461471ea28fc8e0be459 1.26MB / 1.26MB 1.1s done
#6 sha256:4fe16fa8f46966191d59cfcabfff137a623b3cdda747d387bd85dcbf0feff3dd 24.94MB / 39.66MB 2.7s
#6 sha256:4fe16fa8f46966191d59cfcabfff137a623b3cdda747d387bd85dcbf0feff3dd 27.26MB / 39.66MB 2.9s
#6 sha256:4fe16fa8f46966191d59cfcabfff137a623b3cdda747d387bd85dcbf0feff3dd 29.36MB / 39.66MB 3.1s
#6 sha256:4fe16fa8f46966191d59cfcabfff137a623b3cdda747d387bd85dcbf0feff3dd 33.55MB / 39.66MB 3.5s
#6 sha256:4fe16fa8f46966191d59cfcabfff137a623b3cdda747d387bd85dcbf0feff3dd 36.70MB / 39.66MB 3.8s
#6 sha256:4fe16fa8f46966191d59cfcabfff137a623b3cdda747d387bd85dcbf0feff3dd 39.66MB / 39.66MB 4.1s
#6 sha256:4fe16fa8f46966191d59cfcabfff137a623b3cdda747d387bd85dcbf0feff3dd 39.66MB / 39.66MB 4.2s done
#6 extracting sha256:4fe16fa8f46966191d59cfcabfff137a623b3cdda747d387bd85dcbf0feff3dd
#6 extracting sha256:4fe16fa8f46966191d59cfcabfff137a623b3cdda747d387bd85dcbf0feff3dd 5.6s
#6 ...

#7 [internal] load build context
#7 transferring context: 51.28MB 8.6s
#7 ...

#6 [builder 1/6] FROM docker.io/library/node:18-alpine@sha256:974afb6cbc0314dc6502b14243b8a39fbb2d04d975e9059dd066be3e274fbb25
#6 extracting sha256:4fe16fa8f46966191d59cfcabfff137a623b3cdda747d387bd85dcbf0feff3dd 8.0s done
#6 extracting sha256:fc4eb59aab89271acc5a8bd8e5e96fc17af489976964461471ea28fc8e0be459 0.7s done
#6 extracting sha256:e668eba0f82bcfec9fb0cd787bd7f3d013a1266567b092e90ff8b3d3be41807c done
#6 DONE 14.5s

#7 [internal] load build context
#7 ...

#8 [builder 2/6] WORKDIR /client
#8 DONE 0.4s

#7 [internal] load build context
#7 transferring context: 92.14MB 13.7s
#7 transferring context: 118.19MB 17.0s done
#7 DONE 18.4s

#9 [builder 4/6] RUN npm install
#9 CACHED

#10 [builder 3/6] COPY package.json package-lock.json ./
#10 CACHED

#11 [builder 5/6] COPY . .
#11 DONE 30.0s

#12 [builder 6/6] RUN npm run build
#12 1.196 
#12 1.196 > client@0.0.0 build
#12 1.196 > tsc -b && vite build
#12 1.196 
#12 27.13 vite v6.0.11 building for production...
#12 27.69 transforming...
#12 81.81 âœ“ 984 modules transformed.
#12 87.34 rendering chunks...
#12 87.86 computing gzip size...
#12 88.14 dist/index.html                   0.46 kB â”‚ gzip:   0.30 kB
#12 88.14 dist/assets/react-CHdo91hT.svg    4.13 kB â”‚ gzip:   2.05 kB
#12 88.14 dist/assets/index-kQJbKSsj.css    0.92 kB â”‚ gzip:   0.50 kB
#12 88.14 dist/assets/index-CwaP3ILl.js   363.55 kB â”‚ gzip: 118.59 kB
#12 88.14 âœ“ built in 1m 1s
#12 DONE 93.2s

#5 [stage-1 1/3] FROM docker.io/library/nginx:alpine@sha256:814a8e88df978ade80e584cc5b333144b9372a8e3c98872d07137dbf3b44d0e4
#5 CACHED

#13 [stage-1 2/3] COPY --from=builder /client/dist /usr/share/nginx/html
#13 DONE 0.9s

#14 [stage-1 3/3] COPY nginx.conf /etc/nginx/conf.d/default.conf
#14 DONE 1.4s

#15 exporting to image
#15 exporting layers
#15 exporting layers 1.1s done
#15 writing image sha256:05212711c2c98cc5982a9816186dc8fe6f0a2269e762cf38e223016d77f8a3e7 0.0s done
#15 naming to docker.io/library/kvis-client:latest
#15 naming to docker.io/library/kvis-client:latest 0.2s done
#15 DONE 1.4s

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/255wo8tqdss0a5cgpxrcshhmp
The push refers to repository [k3d-local-registry:5001/kvis-client]
663d08af06f3: Preparing
59095c28174a: Preparing
a81f475c8ce2: Preparing
856b39837f15: Preparing
20ca17bb1caf: Preparing
3bc38db2acdb: Preparing
e1482d480729: Preparing
cbb38b57f140: Preparing
b2910501c843: Preparing
534a70dc8296: Preparing
3bc38db2acdb: Waiting
e1482d480729: Waiting
cbb38b57f140: Waiting
b2910501c843: Waiting
534a70dc8296: Waiting
a81f475c8ce2: Layer already exists
856b39837f15: Layer already exists
20ca17bb1caf: Layer already exists
e1482d480729: Layer already exists
cbb38b57f140: Layer already exists
3bc38db2acdb: Layer already exists
534a70dc8296: Layer already exists
b2910501c843: Layer already exists
59095c28174a: Pushed
663d08af06f3: Pushed
latest: digest: sha256:26bbba641fb2f673331adfb5196d4453488ddae2c73d550b91a4509411ba89ca size: 2406
The push refers to repository [k3d-local-registry:5001/kvis-backend]
7845c1242cf7: Preparing
f5cd714b46dc: Preparing
5f70bf18a086: Preparing
89b59b1b7cb3: Preparing
7845c1242cf7: Layer already exists
f5cd714b46dc: Layer already exists
89b59b1b7cb3: Layer already exists
5f70bf18a086: Layer already exists
latest: digest: sha256:f871dfb4e15067dc54f7ecf4451ed0d1687ce58a560c89f2bed85fc163acf478 size: 1152
ingress.networking.k8s.io/kvis-client created
ingress.networking.k8s.io/kvis-backend created
deployment.apps/kvis-client created
service/kvis-client created
deployment.apps/kvis-backend created
service/kvis-backend created
persistentvolumeclaim/dummy-pvc created
#0 building with "desktop-linux" instance using docker driver

#1 [internal] load .dockerignore
#1 transferring context: 0.1s
#1 transferring context: 2B 0.1s done
#1 DONE 3.6s

#2 [internal] load build definition from Dockerfile
#2 transferring dockerfile: 2B 0.2s done
#2 DONE 3.6s
ERROR: failed to run Build function: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount1626265893/Dockerfile: no such file or directory

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/x2w6o5vd3cu2kgcrgc35zxsng
Using default tag: latest
The push refers to repository [k3d-local-registry:5001/curl-image]
acd28f705bdc: Preparing
89b59b1b7cb3: Preparing
89b59b1b7cb3: Mounted from kvis-backend
acd28f705bdc: Pushed
latest: digest: sha256:61210e8f2b072c97c8e6c2efc033b3a6be796a888193f0e17a2b1010ffdf5062 size: 738
error: the path "curl_pod.yaml" does not exist
